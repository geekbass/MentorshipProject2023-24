{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6452076-0fbd-4daa-a8f3-358fe600f06f",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "We are going to train a sentiment analysis Model to determine if a Yelp review is either Positive Or Negative.\n",
    "\n",
    "This model training session is also going to be used as a case study to see how accurate the XGBoost Framework can be used in this type of scenario. This is typically not an NLP framework used.\n",
    "\n",
    "This session will teach the importance of Preprocessing data as well as how to train a model. We will use the popular nltk project for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72715269-fd11-416c-b973-0d6d4f2acd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional libraries\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e562a78-5bec-4da0-b1ad-169d29d94261",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESTART KERNEL FoR NEW LIBRARY TO TAKE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252da9a-6ca0-4013-a4a2-df8e0b84b511",
   "metadata": {},
   "source": [
    "# Preparing our Data\n",
    "We just created a dataset and saved it as 'training_data.csv' in Session 2. We are going to use this to train our model. We will need to clean up and preprocess our data. We will use nltk for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e75a2-e0e8-4be4-a525-601349d88728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dataset to Pandas DF\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('training_data.csv', index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0e761-771f-483d-9338-4d016588dbf0",
   "metadata": {},
   "source": [
    "# Preprocess with nltk\n",
    "We will use stopwords and WordNetLemmatizer.\n",
    "\n",
    "**Stop words** - Stop words are a set of commonly used words in a language. Examples of stop words in English are “a,” “the,” “is,” “are,” etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information.\n",
    "\n",
    "**Word Lemmatizer** - Reduce a word to its root form, also called a lemma. For example, the verb \"running\" would be identified as \"run.\" Lemmatization studies the morphological, or structural, and contextual analysis of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2cd5ba-f9ee-4d98-9017-ef7b9c621af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download corpora (shit ton of text)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# English stop words here\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a01d9-ccc2-4dca-a06d-44d56aaa9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that cleans up the text. Remove things like punctuation, convert to lowercase, lemmatize and remove stop words\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE)\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Lemmatize\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")] \n",
    "    # remove stop words\n",
    "    text = [word for word in text if not word in stop_words] \n",
    "    # Bring the list back into a string\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e94b8c-bad9-4fa2-8b0f-4fe7a78a7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning function above to each row (lambda) of our dataset \n",
    "# Here we add an additional row just to demonstrate.\n",
    "# This is the column we will train our model on!\n",
    "data['cleaned_text'] = data.text.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf524395-4918-4412-9c82-2382b10cb805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read it an notice the differences between text and cleaned_text columns\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a02238-5eb6-47fe-9a01-f3ff3978ee5a",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "In natural language processing (NLP), feature extraction is a fundamental task that involves converting raw text data into a format that can be easily processed by machine learning algorithms.\n",
    "\n",
    "Machines need read numerical data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707e767-a729-434a-a152-b7c8771369b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Sklearn feature extractor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "# Vectorize (convert to numbers) on the cleaned_text column and make it an array to pass to our model for training\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfid_obj = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Save our fitted vectorizer using pickle\n",
    "vec_file = 'vectorizer.pickle'\n",
    "pickle.dump(vectorizer, open(vec_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac7e13-0c87-4e29-b888-04180c509956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the features to an array and Read the array just to see\n",
    "data_features = tfid_obj.toarray()\n",
    "data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604451e-25d4-4346-ac1e-b6697561be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to make our sentiments a numerical value. To keep this simple we will make positive a 1 and negative a 0.\n",
    "def encode_sentiment(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        sentiment_value = 1\n",
    "    else:\n",
    "        sentiment_value = 0\n",
    "\n",
    "    return int(sentiment_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47103f3d-0fec-48d6-ba7f-b6bb53a9b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column with encodings \n",
    "data['encoded_sentiment'] = data.sentiment.apply(lambda x: encode_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb2b17-284c-4909-be47-4ec45a9e94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014dfeac-5bf2-4c91-9559-0e45a4c99630",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "The train-validation-test split is a strategy that divides a dataset into three essential subsets: the training set, the validation set, and the test set. Each subset serves a distinct purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408a1d5-a241-48db-bb16-f741dbb2af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test will be our array above\n",
    "# y_train, y_test is our encoded_sentiment. y is the result (prediction_ you are going for)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_features, data['encoded_sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395471a4-7e8f-41fe-aa98-d51d9be7cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed5b02-437c-4b69-8ad2-8fec27c72810",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Now the fun part!!!! Train an model!!!\n",
    "\n",
    "Now we are going to train an XGBoost model our our training data (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a91530-7a21-4d5f-ba86-7dec7dd5a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Use the Classifier. We can adjust the parameters here to try to get better results.\n",
    "model = xgb.XGBClassifier(max_depth=10, n_estimators=1000, learning_rate=0.01)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b17aa-aa67-4dc0-8f40-fc8695144974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our model locally\n",
    "model.save_model(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65318f82-564d-4c7a-a977-6430832dec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can do model evaluations using our Test Dataset\n",
    "# Use the model to run predictions across 2000K rows\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39536f1a-d7a8-4e01-b518-4dacf5a9133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will output the encoded sentiments\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9315aac-5255-4aab-91b4-f10d8f4d1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several ways we can see the results with different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4c392-f31b-4fc4-9b3d-a69ce09ff576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5706d75-de90-4436-883f-340c4568a92c",
   "metadata": {},
   "source": [
    "# Inference (Real Tests)\n",
    "Now lets run through the process of using some real text to get predictions. We must follow the same steps we did for training as we do to get results.\n",
    "\n",
    "**Inference** - Applying a machine learning model to a dataset and generating an output or “prediction”. This output might be a numerical score, a string of text, an image, or any other structured or unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb03dd-c3d7-436e-9b52-1447a7f0c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model if needed\n",
    "# import xgboost as xgb\n",
    "\n",
    "# model = xgb.XGBClassifier(max_depth=10, n_estimators=1000, learning_rate=0.01)\n",
    "# model.load_model(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1543c4b-f681-46d6-8361-572865d1f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow same steps as dataprocessing for model\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download corpora (shit ton of text)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# English stop words here\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7d4f2-b650-403a-a7d4-bc797daf8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our vectorizer\n",
    "loaded_vectorizer = pickle.load(open('vectorizer.pickle', 'rb'))\n",
    "\n",
    "def clean_and_vectorize(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE)\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Lemmatize\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")] \n",
    "    # remove stop words\n",
    "    text = [word for word in text if not word in stop_words] \n",
    "    # Bring the list back into a string\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Vectorize from our vectorizer created above\n",
    "    data_features = loaded_vectorizer.transform([text])\n",
    "    # Create an array as it expects\n",
    "    data_features = data_features.toarray()\n",
    "\n",
    "    # Get the prediciton \n",
    "    prediction = model.predict(data_features)[0]\n",
    "\n",
    "    # 1 is positive 0 is negative\n",
    "    if prediction == 1:\n",
    "        sentiment = 'positive'\n",
    "    else: \n",
    "        sentiment = 'negative'\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f87dd1-8a13-4eab-9c2b-b67f787f5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This place is the greatest on earth!'\n",
    "prediction = clean_and_vectorize(text)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08174e-453f-418c-b9ff-9d89ebfcc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Alqueria surpassed my expectations ten fold. You can tell that their food is authentic farm-to-table and is just incredibly fresh.\n",
    "We ordered the shrimp and drunken goat cheese as our appetizers and they proved we made the right decision in choosing Alqueria for dinner. The shrimp, with the oil that it is in, is unrivaled. I would come back just to eat more of this! The lamb and spaghetti squash were also very good. The lamb fell right off the bone.\n",
    "Our server was very sweet to us, offered her suggestions, and always checked in.\n",
    "I do love the intimate feel of the restaurant, however, reservations are necessary since it is a smaller place.\n",
    "The menu changes often which I think is a fun concept, but I am really hoping they don't ever take the shrimp off the menu!\n",
    "Braised Lamb Shank & Local Spaghetti Squash\n",
    "\"\"\"\n",
    "\n",
    "prediction = clean_and_vectorize(text)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b88ac5-945b-48ca-91cf-e04d8ac37af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Wow! Downright awful! Some background: I have avoided eating Panera basically my entire life because it's terrible. However! In Orlando a group of friends wanted to go to Panera, so I went. It was awesome! So good that I told several people about it and was like we have to go when we're all back in Columbus because it was so good. Which was so surprising. Well. Here we are. And it was awful. Terrible. Put together terribly and tasted like feet and rot. Stay away. Better to not eat than to waste money on this garbage.\n",
    "\"\"\"\n",
    "prediction = clean_and_vectorize(text)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191aa1e-4a3e-4d3b-8050-ed6aede32738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
